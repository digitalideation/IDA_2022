## Data Self Portrait – The Quantified Self

### Tagesaufgabe
Erarbeite eine Darstellung mit p5.js, bei der dein Gesicht in irgendeiner Form eine Rolle spielt. 

**_Inspiration_**
* http://www.generative-gestaltung.de/2/
* https://tinyurl.com/bdz437y4  Alex Dragulescu, lexigraphs
* https://medium.com/swlh/new-designs-for-living-online-ffb541711757
* https://github.com/shiffman/Face-It
* https://rachelbinx.com/project/wifidiary


***
# Morgen
* Zugriff auf die Kamera in p5.js
* das Pixel Array
* Extended: Arbeit mit ml5.js

## Tutorial Kamera
Mit p5.js könnt ihr sehr einfach auf die Kamera zugreifen. 
```js
// tutorial
// https://www.youtube.com/watch?v=rNqaw8LT2ZU
let capture; // variable für kamera

function setup() {
    createCanvas(640, 480);
    pixelDensity(1);
    background(100);

    capture = createCapture(VIDEO); // https://p5js.org/reference/#/p5/createCapture
    capture.size(640, 480);
    capture.hide();
}

function draw() {
    capimg = capture.get(); // videostream in ein aktuelles Einzelbild kopieren
    image(capimg, 0, 0, width, height); //aktuelles Einzelbild darstellen
}
```
<b>Bild per Klick speichern, Daten in JSON schreiben</b> <br/><br/>
```js
function keyReleased() {
    if (key == 's' || key == 'S') {
        let d = new Date();
        let now = d.getFullYear() + "" + (d.getMonth() + 1) + "" + d.getDate() + "" + (d.getHours() + 1) + "-" + (d.getMinutes() + 1) + "" + (d.getSeconds() + 1) + "-" + frameCount;
        saveCanvas(now, 'png');
    }
}
```
<b>Daten in JSON schreiben</b> <br/><br/>
```js
function keyReleased() {
    if (key == 's' || key == 'S') {
        let d = new Date();
        let now = d.getFullYear() + "" + (d.getMonth() + 1) + "" + d.getDate() + "" + (d.getHours() + 1) + "-" + (d.getMinutes() + 1) + "" + (d.getSeconds() + 1) + "-" + frameCount;
        saveCanvas(now, 'png');

        saveJSFile(now);//falls ihr noch ein JSON File generieren wollt
    }
}
```

Der Code oben stellt euch den Videostream 1:1 dar. <br/>
Ihr könnt aber auf jeden einzelnen Pixel zugreifen und den verändern. <br/>
Informationen zum Pixel Array: https://docs.google.com/presentation/d/104VbNZyDklRJWsJmG86VUrvtpTb2S4PbhuwkliPYxVI/edit#slide=id.p<br/>
Pro Pixel gibt es vier Einträge im Array: https://docs.google.com/presentation/d/104VbNZyDklRJWsJmG86VUrvtpTb2S4PbhuwkliPYxVI/edit#slide=id.g1175de9062b_0_80 <br/>
Der folgende Code berechnet für jeden Pixel die Helligkeit aus rot, grün, blau und schreibt den Helligkeitswert zurück ins Array. Folge ist ein Bild in Grauwerten.

```js
function draw() {
    capimg = capture.get(); // videostream kopieren
    

    if (capimg.width > 0) {
         capimg.loadPixels(); // alle pixel in das array capimg.pixels laden

         for (let i = 0; i < capimg.pixels.length; i += 4) { //mit 4 hochzählen, weil es zu jedem Pixel vier Angaben hat
             let r = capimg.pixels[i + 0];//rot
             let g = capimg.pixels[i + 1];//gruen
             let b = capimg.pixels[i + 2];//blau
             let a = capimg.pixels[i + 3];//alpha

             let bright = (r + g + b) / 3;//aus rgb grauwert berechnen

             capimg.pixels[i + 0] = bright; //rgb Kanäle je auf denselben Wert (Grauwert) setzen
             capimg.pixels[i + 1] = bright;
             capimg.pixels[i + 2] = bright;
         }
         capimg.updatePixels(); //neue werte zurück schreiben in capimg.pixels
         
     }

     image(capimg, 0, 0, width, height);
}
```
Mit der Möglichkeit, auf jeden Pixel zuzugreifen und pro Pixel die RGBA Werte einzeln zu manipulieren, könnt ihr eigene Filter erstellen. Bsp. Kanäle tauschen, umkehren, etc. 
Hier ein kleines Beispiel, das einen Schwarz-Weiss Filter baut. Die Pixel sind entweder voll schwarz (falls Helligkeit unter 100) oder voll weiss (falls Helligkeit über 99).
Probiert einen eigenen Filter zu bauen. 

```js
 if (bright < 100) {
                capimg.pixels[i + 0] = 0;
                capimg.pixels[i + 1] = 0;
                capimg.pixels[i + 2] = 0;
            } else {
                capimg.pixels[i + 0] = 255;
                capimg.pixels[i + 1] = 255;
                capimg.pixels[i + 2] = 255;
            }
```
Extended: <br/>
Daniel Shiffman zeigt hier, wie ihr ein Rasterbild baut. https://www.youtube.com/watch?v=m1G6WBvrOBE <br/>
Ihr lest eine verkleinerte Version des Kamerastreams ein (braucht weniger Speicher und Performance) und bläst dann diese Version in einzelnen Rasterpunkten auf. <br/>
Schaut das Video und versucht, das nachzubauen. <br/>
Erklärungen, wie x und y Position mit dem Schlüssel im Pixel Array zusammenhängen, hier: https://docs.google.com/presentation/d/104VbNZyDklRJWsJmG86VUrvtpTb2S4PbhuwkliPYxVI/edit#slide=id.g1175de9062b_0_35 <br/><br/>

Die Rasterpunkte könnten nun irgendetwas sein, auch Buchstaben, Icons, etc. <br/>
Entwirf deine Version. <br/><br/>


## Gesichtserkennung
### Google API
https://cloud.google.com/vision/docs/drag-and-drop?hl=de
### ML5
https://ml5js.org/about <br/>
https://learn.ml5js.org/ <br/>
ml5.js ist inspiriert von p5.js. Die Bibliothek wird durch Codebeispiele, Tutorien und Beispieldatensätze unterstützt, wobei der Schwerpunkt auf ethischer Datenverarbeitung liegt. Voreingenommenheit in Daten, stereotype Schäden und verantwortungsvolles Crowdsourcing sind Teil der Dokumentation zur Datenerfassung und -nutzung. 
<br/>
Ein Beispiel, das Mund, Nase, Kinn, Augen, etc. erkennt. Und sie in der Farbe zeichnet, die an der Stelle im Video ist. 
Ihr könnt euch auch mal das Resultat (in der Variable detections) ausgeben lassen, es ist ein JSON Object mit den Formen und Punkten. 

```js
let faceapi;
let video;
let detections;

// by default all options are set to true
const detection_options = {
    withLandmarks: true,
    withDescriptors: false,
}


function setup() {
    createCanvas(360, 270);

    // load up your video
    video = createCapture(VIDEO);
    video.size(width, height);

    video.hide(); // Hide the video element, and just show the canvas
    faceapi = ml5.faceApi(video, detection_options, modelReady)
    textAlign(RIGHT);
    frameRate(1);
}

function modelReady() {
    console.log('ready!')
    console.log(faceapi)
    faceapi.detect(gotResults)

}

function gotResults(err, result) {
    if (err) {
        console.log(err)
        return
    }
    // console.log(result)
    detections = result;

    // background(220);
    background(255, 150);
    video.loadPixels();
    //image(video, 0, 0, width, height)
    if (detections) {
        if (detections.length > 0) {
            // console.log(detections)
            //drawBox(detections)
            drawLandmarks(detections)
        }

    }
    faceapi.detect(gotResults)
}

function drawBox(detections) {
    for (let i = 0; i < detections.length; i++) {
        const alignedRect = detections[i].alignedRect;
        const x = alignedRect._box._x
        const y = alignedRect._box._y
        const boxWidth = alignedRect._box._width
        const boxHeight = alignedRect._box._height

        noFill();
        stroke(161, 95, 251);
        strokeWeight(2);
        rect(x, y, boxWidth, boxHeight);
    }

}

function drawLandmarks(detections) {
    //noFill();
    stroke(161, 95, 251)
    strokeWeight(2)
        //console.log(detections);
    for (let i = 0; i < detections.length; i++) {
        const mouth = detections[i].parts.mouth;
        const nose = detections[i].parts.nose;
        const leftEye = detections[i].parts.leftEye;
        const rightEye = detections[i].parts.rightEye;
        const rightEyeBrow = detections[i].parts.rightEyeBrow;
        const leftEyeBrow = detections[i].parts.leftEyeBrow;
        const landMarks = detections[i].unshiftedLandmarks._positions;
        const jawOutline = detections[i].parts.jawOutline;

        //drawPart(landMarks, true);
        drawPart(jawOutline, false);
        drawPart(mouth, true);
        drawPart(nose, false);
        drawPart(leftEye, true);
        drawPart(leftEyeBrow, false);
        drawPart(rightEye, true);
        drawPart(rightEyeBrow, false);

    }

}

function drawPart(feature, closed) {


    beginShape();
    for (let i = 0; i < feature.length; i++) {
        const x = feature[i]._x
        const y = feature[i]._y

        //get Color from Pixel 
        let n = (int(y) * video.width + int(x)) * 4;
        let r = video.pixels[n + 0];
        let g = video.pixels[n + 1];
        let b = video.pixels[n + 2];
        let a = video.pixels[n + 3];



        stroke(r, g, b, a);
        //fill(r, g, b, a);
        //ellipse(x, y, 4, 4);
        vertex(x, y)
    }

    if (closed === true) {
        endShape(CLOSE);
    } else {
        endShape();
    }

}
``` 

## Links
* https://matlo.me/
* https://magenta.tensorflow.org/demos/web/
* https://controllers.ai/
